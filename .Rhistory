train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus)
plot(val_E~nus, type = "l")
plot(val_E~nus, type = "l", xlab = expression(n),
ylab = "Cross-Entropy Error")
M_seq = 10
nus   = exp(seq(-10, 0, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(n),
ylab = "Cross-Entropy Error", lwd = 2)
set.seed(2002)
M_seq = 10
nus   = exp(seq(-10, 0, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(n),
ylab = "Cross-Entropy Error", lwd = 2)
set.seed(2002)
M_seq = 10
nus   = exp(seq(-10, 0, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(n),
ylab = "Cross-Entropy Error", lwd = 2)
set.seed(2002)
M_seq = 15
nus   = exp(seq(-10, 0, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(n),
ylab = "Cross-Entropy Error", lwd = 2)
set.seed(2002)
M_seq = 15
nus   = exp(seq(-10, 1, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
val_E
set.seed(4026)
M_seq = 15
nus   = exp(seq(-10, 1, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
set.seed(4026)
M_seq = 20
nus   = exp(seq(-10, 1, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
set.seed(4026)
M_seq = 20
nus   = exp(seq(-20, 1, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
set.seed(4026)
M_seq = 20
nus   = exp(seq(-20, 1, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
train_E
val_E
train_E
set.seed(4026)
M_seq = 20
nus   = exp(seq(-20, 1, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
set.seed(4026)
M_seq = 12
nus   = exp(seq(-20, 1, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
set.seed(4026)
M_seq = 12
nus   = exp(seq(-20, 0, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
rm(list = ls9)
rm(list = ls())
library(kableExtra)
library(broom)
library(knitr)
cancerA = read.table("Breast Cancer Dataset A.txt", header = T)
kable(head(cancerA, n = 3), caption = "Breast Cancer Dataset A") %>%
kable_styling(full_width = F)
# Function to calculate the number of parameters in (m,m)-network
npars = function(X, Y, d = c(m, m))
{
X = cbind(1, X)
p = ncol(X)
q = ncol(Y)
m = d[1]
d = d[2]
return(p*m + m*d + d*q + m + d + p)
}
# Activation Functions
sig1 = function(z) # Hidden Layers
{
tanh(z)
}
sig2 = function(z)
{
tanh(z)
}
sig3 = function(z) # Output Layer
{
1/(1+exp(-z))
}
# Cost Funtion
g = function(AL)   # Cross-Entropy
{
Y  = matrix(Y, ncol = 1) ; AL = matrix(AL, ncol = 1)
Error = mean(ifelse(Y == 1, Y*log(AL), (1-Y)*log(1 - AL)))
return(-Error)
}
neural_net = function(X, Y, theta, d = c(m, m), nu)
{
# Design Matrix and dimensions
X = cbind(1, X)
p = ncol(X)
N = nrow(Y)
q = ncol(Y)
m = d[1]
d = d[2]
ones = matrix(1, ncol = 1, nrow = N)
# The Weights and Biases
index = 1:(p*m)
W1    = matrix(theta[index], nrow = p, ncol = m)
index = max(index):(m*d)
W2    = matrix(theta[index], nrow = m, ncol = d)
index = max(index):(d*q)
W3    = matrix(theta[index], nrow = d, ncol = q)
index = max(index):m
b1    = matrix(theta[index], nrow = m, ncol = 1)
index = max(index):d
b2    = matrix(theta[index], nrow = d, ncol = 1)
index = max(index):q
b3    = matrix(theta[index], ncol = 1, nrow = q)
# Evaluating the Network
A0 = as.matrix(t(X))
A1 = sig1(t(W1)%*%A0 + b1%*%t(ones))
A2 = sig2(t(W2)%*%A1 + b2%*%t(ones))
A3 = sig3(t(W3)%*%A2 + b3%*%t(ones))
A3 = t(A3)
# Error and Predictions
E1 = -mean(ifelse(Y == 1, Y*log(A3), (1-Y)*log(1 - A3)))
E2 = E1 + (nu/N)*(sum(abs(W1)) + sum(abs(W2)) + sum(abs(W3)))
Yhat = ifelse(A3 >= 0.5, 1, 0)
return(list(Yhat = Yhat, out = A3, E1 = E1, E2 = E2))
}
X = as.matrix(cancerA[,1:5])
Y = as.matrix(cancerA[, 6], ncol = 1)
N = nrow(X)
m = 5 # for (m, m)-network
# Validation and Training Data
set = sample(1:N, 0.8*N, replace = F)
train_x = as.matrix(X[set, ])
train_y = as.matrix(Y[set, ], ncol = 1)
val_x   = as.matrix(X[-set, ])
val_y   = as.matrix(Y[-set], ncol = 1)
val_y
val_x
train_y
train_x
obj = function(parameters)
{
res = neural_net(train_x, train_y, theta_rand, d = c(m,m), nu = nu)
return(res$E2)
}
set.seed(4026)
M_seq = 12
nus   = exp(seq(-20, 0, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
theta_rand = runif(npars(train_x, train_y, c(m, m)), -1, 1)
resReg = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
npars(train_x, train_y, c(m, m))
obj = function(parameters)
{
res = neural_net(train_x, train_y, parameters, d = c(m,m), nu = nu)
return(res$E2)
}
set.seed(4026)
M_seq = 12
nus   = exp(seq(-20, 0, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
npar       = npars(train_x, train_y, c(m, m))
theta_rand = runif(npar, -1, 1)
resReg     = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
set.seed(4026)
M_seq = 12
nus   = exp(seq(-10, -3, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
npar       = npars(train_x, train_y, c(m, m))
theta_rand = runif(npar, -1, 1)
resReg     = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
set.seed(4026)
M_seq = 10
nus   = exp(seq(-10, -3, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
npar       = npars(train_x, train_y, c(m, m))
theta_rand = runif(npar, -1, 1)
resReg     = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
set.seed(4026)
M_seq = 8
nus   = exp(seq(-10, 0, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
npar       = npars(train_x, train_y, c(m, m))
theta_rand = runif(npar, -1, 1)
resReg     = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
set.seed(4026)
M_seq = 8
nus   = exp(seq(-10, 1, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
npar       = npars(train_x, train_y, c(m, m))
theta_rand = runif(npar, -1, 1)
resReg     = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
set.seed(4026)
M_seq = 8
nus   = exp(seq(-10, 2, length = M_seq))
train_E = rep(NA, M_seq)
val_E   = rep(NA, M_seq)
theta   = list()
for(i in 1:M_seq)
{
nu = nus[i]
npar       = npars(train_x, train_y, c(m, m))
theta_rand = runif(npar, -1, 1)
resReg     = nlm(obj, theta_rand, iterlim = 500)
res1 = neural_net(train_x, train_y, resReg$estimate, c(m,m), 0)
res2 = neural_net(val_x, val_y, resReg$estimate, c(m,m), 0)
theta[[i]] = resReg$estimate
train_E[i] = res1$E1
val_E[i]   = res2$E1
}
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
lines(train_E~nus, lwd = 2, col = "darkgreen")
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
abline(v = nus[which.min(val_E)])
plot(val_E~nus, type = "l", xlab = expression(nu),
ylab = "Cross-Entropy Error", lwd = 2)
abline(v = nus[which.min(val_E)], lty = 3)
setwd("~/Masande/Final Year/Project Thesis/Presentation Round II")
